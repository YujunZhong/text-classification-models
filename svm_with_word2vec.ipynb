{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCo5ChcKIoMy",
        "outputId": "35077175-c7d9-4a01-ee35-dae74402da61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 15.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import contractions\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "def text_preprocessing(df):\n",
        "    \"\"\" Includes all data preprocessing.\n",
        "    :param df: raw dataframe\n",
        "    :param df: preprocessed data\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    df['new_text']=df['text'].apply(lambda x: contractions.fix(x, slang=True))\n",
        "    df['new_text'] = df['new_text'].str.lower()\n",
        "    df['new_text'] = df['new_text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '' , x))\n",
        "    df['new_text'] = df['new_text'].apply(lambda x: re.sub(r'\\w*\\d\\w*', '', x))\n",
        "    df['new_text'] = df['new_text'].apply(lambda x: re.sub(' +', ' ', x))\n",
        "    return df"
      ],
      "metadata": {
        "id": "PwlqR_frId2d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the original data. Note that we need to put the data file in the same folder with this notebook"
      ],
      "metadata": {
        "id": "CkjiVaNFWCC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "texts_df = pd.read_csv(\"train_data.csv\")    # should be adjusted according to the position of data files\n",
        "labels_df = pd.read_csv(\"train_results.csv\")  # should be adjusted according to the position of data files\n",
        "texts_df['label'] = labels_df.apply(lambda row: label_dict[row['target']], axis=1)\n",
        "texts_df_new = text_preprocessing(texts_df)\n",
        "tmp_corpus = texts_df_new['new_text'].map(lambda x: x.split('.'))\n"
      ],
      "metadata": {
        "id": "6HfU3lpYTp67"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construc the corpus using the existing data"
      ],
      "metadata": {
        "id": "YW_YhT_pWGmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "corpus = []\n",
        "for i in tqdm(range(len(tmp_corpus))):\n",
        "    for line in tmp_corpus[i]:\n",
        "        words = [x for x in line.split()]\n",
        "        corpus.append(words)\n",
        "num_of_sentences = len(corpus)\n",
        "num_of_words = 0\n",
        "for line in corpus:\n",
        "    num_of_words += len(line)\n",
        "\n",
        "print('Num of sentences - %s'%(num_of_sentences))\n",
        "print('Num of words - %s'%(num_of_words))"
      ],
      "metadata": {
        "id": "ByxaR7xfWYKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84f1ebd6-eb45-4d6c-c965-dfd8c53eabf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1040323/1040323 [00:09<00:00, 111115.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of sentences - 1040323\n",
            "Num of words - 13738975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training our own word2vec model"
      ],
      "metadata": {
        "id": "9gBQX8KEWLhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 100\n",
        "window_size = 2 # sentences weren't too long, so\n",
        "epochs = 5\n",
        "min_count = 2\n",
        "workers = 4\n",
        "import random\n",
        "# shuffle corpus\n",
        "def shuffle_corpus(sentences):\n",
        "    shuffled = list(sentences)\n",
        "    random.shuffle(shuffled)\n",
        "    return shuffled\n",
        "# train word2vec model using gensim\n",
        "model = Word2Vec(corpus, sg=1,window=window_size,size=size,\n",
        "                 min_count=min_count, workers=workers, iter=epochs, sample=0.01)\n",
        "model.build_vocab(sentences=shuffle_corpus(corpus),update=True)\n",
        "\n",
        "model.train(sentences=shuffle_corpus(corpus),epochs=2,total_examples=model.corpus_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0YYOsj2WpYN",
        "outputId": "2872e124-9649-43b9-8cda-fd47cf8cdf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(26246538, 27477950)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the word2vec model"
      ],
      "metadata": {
        "id": "SpMJuveqWVPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('w2v_model_100')\n",
        "# from gensim.models import Word2Vec\n",
        "# model = Word2Vec.load('w2v_model_100')"
      ],
      "metadata": {
        "id": "xe2-dwS7XMF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the word2vec model to tokenize our sentances"
      ],
      "metadata": {
        "id": "APLQnYc4WWRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def FunctionText2Vec(corpus):\n",
        "    \"\"\" Use the word2vec model to tokenize our sentances.\n",
        "    :param corpus: cleaned text data\n",
        "    :return: tokenized data\n",
        "    \"\"\"\n",
        "    vector_Data=[]\n",
        "\n",
        "    # Looping through each row for the data\n",
        "    for sentence in corpus:\n",
        "        # initiating a sentence with all zeros\n",
        "        vector_sentence = np.zeros(100)\n",
        "        # Looping thru each word in the sentence and if its present in \n",
        "        # the Word2Vec model then storing its vector\n",
        "        for word in sentence:\n",
        "            #print(word)\n",
        "            if word in model.wv.vocab:    \n",
        "                vector_sentence=vector_sentence+model.wv[word]\n",
        "        # Appending the sentence to the dataframe\n",
        "        vector_Data.append(vector_sentence.tolist())\n",
        "\n",
        "\n",
        "    return vector_Data\n",
        "all_data = FunctionText2Vec(corpus)"
      ],
      "metadata": {
        "id": "rRKxHxBve21Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def save_model(clf, filename='trained_model_100.pkl'):\n",
        "    pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "7jYdmSkj2zl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the New SVM with vectors generated by word2Vec model"
      ],
      "metadata": {
        "id": "RmJp-1AtW0Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "def train_svm(Train_X, Train_Y, Test_X, Test_Y):\n",
        "    \"\"\" Training SVM and save the model.\n",
        "    :param train_data: transformed training data\n",
        "    :param val_data: transformed validation data\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # linearSVC\n",
        "    text_clf_svm = Pipeline([\n",
        "        # ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
        "        # ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
        "        ('clf-svm', LinearSVC(loss='hinge', C=1.0, class_weight='balanced')),\n",
        "    ])\n",
        "\n",
        "    _ = text_clf_svm.fit(Train_X, Train_Y)\n",
        "    preds = text_clf_svm.predict(Test_X)\n",
        "    acc = np.mean(preds == Test_Y)\n",
        "    print(f'Test accurary of SVM model is: {acc}')\n",
        "\n",
        "    save_model(text_clf_svm, 'svm_trained_model_trigram_100.pkl')\n",
        "\n",
        "train_data_len = int(len(all_data)*0.9)\n",
        "print(train_data_len)\n",
        "print(texts_df_new.columns)\n",
        "\n",
        "Train_X, Train_Y, Test_X, Test_Y = all_data[:train_data_len],texts_df_new.iloc[:train_data_len,2],all_data[train_data_len:],texts_df_new.iloc[train_data_len:,2]\n",
        "train_svm(Train_X, Train_Y, Test_X, Test_Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y26PJ_Kkav_c",
        "outputId": "dd83895d-7ce1-4229-ef6a-0a83ce684583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "832258\n",
            "Index(['id', 'text', 'label', 'new_text'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accurary of SVM model is: 0.7419075769591233\n"
          ]
        }
      ]
    }
  ]
}